{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa022ad6-bd3c-462d-adef-de9d926e99fd",
   "metadata": {},
   "source": [
    "# 1. üé• Demo\n",
    "\n",
    "[Click here to watch the demo](demo.mp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ac890-b7ee-49ca-9ce8-be469ba71f52",
   "metadata": {},
   "source": [
    "# 2. Vehicle Counting Repository Setup Guide\n",
    "\n",
    "## Credits:\n",
    "\n",
    "Before we begin, it's important to acknowledge the foundations upon which this project is built. Please take a moment to review the credits and licensing information below. Respecting the AGPL license is crucial before using, consuming, editing, or sharing this repository.\n",
    "\n",
    "**Credits:**\n",
    "\n",
    "* **Object Detection:** This project utilizes the [ultralytics](https://github.com/mikel-brostrom/ultralytics) repository for object detection, licensed under the AGPL license.\n",
    "* **Object Tracking:** This project utilizes the [boxmot](https://github.com/mikel-brostrom/boxmot) repository for object tracking, also licensed under the AGPL license.\n",
    "\n",
    "This notebook provides step-by-step instructions to set up and run the vehicle counting application on four different platforms: Google Colab, Jupyter Notebooks, and via Bash/Linux commands and in  NANO JETSON Kit.\n",
    "## 1.1. Google Colab\n",
    "**Note:** Please don't forget to set the runtime type to **GPU (T4)** in Colab for optimal performance.\n",
    "\n",
    "### Setting the Runtime to GPU (T4):\n",
    "\n",
    "1. After the notebook opens, navigate to the top menu and select **Runtime** > **Change runtime type**.\n",
    "2. In the popup window, set **Hardware accelerator** to **GPU**.\n",
    "3. If available, select **T4** as the GPU type.\n",
    "4. Run the below code cells after setting .yaml config file with target rois, lines and params\n",
    "\n",
    "## 1.2. Jupyter Notebooks\n",
    "\n",
    "Note: In case you want to use the Geoforce GPU in your computer to accelerate to speed up processing, kindly install CUDA in your computer \n",
    "Follow these steps to set up and run the application in Jupyter Notebooks.\n",
    "\n",
    "### Using a GeForce GPU in your computer for Accelerated Processing  \n",
    "\n",
    "To utilize your computer's GeForce GPU to speed up processing, follow these steps:  \n",
    "\n",
    "1. **Install CUDA:**  \n",
    "   Download and install the CUDA toolkit compatible with your GPU from the [NVIDIA CUDA Toolkit Archive](https://developer.nvidia.com/cuda-12-4-0-download-archive).  \n",
    "\n",
    "2. **Install PyTorch with GPU Support:**  \n",
    "   Visit [PyTorch's Get Started Guide](https://pytorch.org/get-started/locally/) to install the appropriate version of PyTorch for your system with GPU (CUDA) support.\n",
    "\n",
    "#### Notes  \n",
    "- Ensure your GPU driver is up-to-date before installing CUDA.  \n",
    "- Follow the instructions on the linked pages carefully to avoid compatibility issues.\n",
    "\n",
    "### Step 1: Create Virtual Environment (Bash/Anaconda Prompt)\n",
    "Open a Bash or Anaconda Prompt and run the following commands to create and activate a virtual environment named `vehicle_counter`:\n",
    "```bash\n",
    "conda create --name vehicle_counter python=3.8\n",
    "conda activate vehicle_counter\n",
    "```\n",
    "This step assumes you have already installed Anaconda in your computer\n",
    "\n",
    "> **Note:** You can neglect the above two instructions if you are NOT working in a virtual environment.\n",
    "\n",
    "### Step 2: Clone the Vehicle Counting Repository\n",
    "Clone the repository and ensure that vehicle-counting-smart-sensor-v2 is set as your working directory if you haven't done so already.\n",
    "\n",
    "```python\n",
    "!git clone https://github.com/hamzaelouiaazzani/vehicle-counting-smart-sensor-v2.git\n",
    "```\n",
    "\n",
    "### Step 3: Upgrade pip and Install Dependencies\n",
    "Download/clone the repository and run the following cell to upgrade pip, setuptools, and wheel, and install the repository dependencies.\n",
    "\n",
    "```python\n",
    "!pip install --upgrade pip setuptools wheel\n",
    "!pip install -e .\n",
    "```\n",
    "> **Note:** Once you run this cell, comment it out and do not run it again because the packages are already installed in your environment.\n",
    "\n",
    "### Step 4: Verify Torch Installation\n",
    "Run the following cell to confirm that NumPy version 1.24.4 is installed, PyTorch is set up, and CUDA is available for GPU support.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "print(\"NumPy Version:\", np.__version__)\n",
    "\n",
    "from IPython.display import Image, clear_output  # to display images in Jupyter notebooks\n",
    "clear_output()\n",
    "\n",
    "import torch\n",
    "print(f\"Cuda availaibility: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")\n",
    "```\n",
    "\n",
    "### Step 5: set the .yaml config file with target rois, lines and params\n",
    "\n",
    "### Step 6: Verify Torch Installation\n",
    "Run the below engine code cell.\n",
    "\n",
    "\n",
    "## 1.3. Running the Repository on NANO JETSON Kit\n",
    "\n",
    "Follow these steps to set up and run the repository on a NANO JETSON Developer Kit with GPU support.\n",
    "\n",
    "### Step 1: Download cuSPARSElt\n",
    "Download cuSPARSElt to enable GPU usage with PyTorch and TorchVision:\n",
    "- Visit the following link: [cuSPARSElt Downloads](https://developer.nvidia.com/cusparselt-downloads?target_os=Linux&target_arch=aarch64-jetson&Compilation=Native&Distribution=Ubuntu&target_version=22.04&target_type=deb_network)\n",
    "- Ensure the file is compatible with your setup.\n",
    "- Make sure the file `cuda-keyring_1.1-1_all.deb` (or a similar one) is successfully downloaded to your JETSON NANO Kit root: \n",
    "\n",
    "### Step 2: Create and Activate a Virtual Environment\n",
    "1. Create a new virtual environment:\n",
    "   ```bash\n",
    "   python3 -m venv vehicle_counter\n",
    "   ```\n",
    "2. Activate the virtual environment:\n",
    "   ```bash\n",
    "   source vehicle_counter/bin/activate\n",
    "   ```\n",
    "\n",
    "### Step 3: Check the NVIDIA Forum\n",
    "Refer to the following NVIDIA forum for compatible PyTorch and TorchVision Wheel files:\n",
    "- [PyTorch for Jetson](https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048)\n",
    "\n",
    "### Step 4: Download Required Files\n",
    "Download the following files:\n",
    "- PyTorch Wheel: `torch-2.3.0-cp310-cp310-linux_aarch64.whl`\n",
    "- TorchVision Wheel: `torchvision-0.18.0a0+6043bc2-cp310-cp310-linux_aarch64.whl`\n",
    "- These files are compatible with JetPack 6.1, Ubuntu 22.04, CUDA 12.6, and Jetson Linux L4T R36.4.\n",
    "\n",
    "If these versions do not work, refer back to the forum for other compatible versions. If no prebuilt versions are suitable, you can build PyTorch and TorchVision from source by using these repositories:\n",
    "- [PyTorch Source Repository]( https://github.com/pytorch/pytorch)\n",
    "- [TorchVision Source Repository](https://github.com/pytorch/vision)\n",
    "\n",
    "\n",
    "### Step 5: Clone the Repository\n",
    "Clone the repository to your Jetson Kit:\n",
    "```bash\n",
    "!git clone https://github.com/hamzaelouiaazzani/vehicle-counting-smart-sensor-v2.git\n",
    "cd vehicle-counting-smart-sensor-v2\n",
    "```\n",
    "\n",
    "### Step 6: Install Dependencies\n",
    "Upgrade pip, setuptools, and wheel, then install the repository dependencies:\n",
    "```bash\n",
    "pip install --upgrade pip setuptools wheel\n",
    "pip install -e .\n",
    "cd ..\n",
    "```\n",
    "\n",
    "### Step 7: Verify Installation\n",
    "Run the following commands to confirm installation:\n",
    "```bash\n",
    "python3 -c \"import torch; print('PyTorch version:', torch.__version__); print('CUDA available:', torch.cuda.is_available())\"\n",
    "python3 -c \"import torchvision; print('TorchVision version:', torchvision.__version__)\"\n",
    "```\n",
    "The installation of PyTorch and TorchVision using the previous instructions enables only CPU functionality on your NANO Jetson Kit, not GPU support. To confirm this, run the following command. If GPU is not enabled, it will display ‚ÄúCUDA available: False‚Äù.\n",
    "\n",
    "### Step 8: Enable GPU Support\n",
    "Install the appropriate PyTorch and TorchVision versions for GPU support:\n",
    "```bash\n",
    "pip3 install torch-2.3.0-cp310-cp310-linux_aarch64.whl\n",
    "pip3 install torchvision-0.18.0a0+6043bc2-cp310-cp310-linux_aarch64.whl\n",
    "pip install numpy==1.24.4\n",
    "```\n",
    "### Step 9: set the .yaml config file with target rois, lines and params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849ed49",
   "metadata": {},
   "source": [
    "# 3. Vehicle Counting ‚Äî Interactive Demo & Visualiser\n",
    "\n",
    "This notebook provides an interactive demo and runner for the **Vehicle Counting Smart Sensor** pipeline (BoXMOT + Ultralytics detector). It preserves all original code cells unchanged ‚Äî the code below is the original work; these top cells are documentation only so you can use this notebook as the project README.\n",
    "\n",
    "Use this notebook to:\n",
    "- Visualise and set geometric shapes (ROIs, lines, polygons) using the interactive selectors.\n",
    "- Run the end-to-end pipeline: frame grabbing ‚Üí detection (Ultralytics) ‚Üí tracking (BoXMOT) ‚Üí counting ‚Üí video output.\n",
    "- Produce `output_counting.mp4` containing the visualised counting demo.\n",
    "\n",
    "Keep the code cells unchanged when running to ensure reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51719be",
   "metadata": {},
   "source": [
    "## Quick Start (short)\n",
    "\n",
    "1. Activate your conda environment (example name `smart_sensor`):\n",
    "   ```bash\n",
    "   conda activate smart_sensor\n",
    "   ```\n",
    "2. Ensure required Python packages are installed (see detailed list below).\n",
    "3. Edit the `source` path in the notebook to point to your video file (example: `C:\\Users\\hamza\\Datasets\\TrafficDatasets\\IMAROC_2\\kech37.mp4`).\n",
    "4. Run the notebook cells from top to bottom (or run the main execution block that starts the pipeline).\n",
    "5. The demo writes `output_counting.mp4` in the working directory.\n",
    "\n",
    "Press `q` in the OpenCV window to stop the run safely (the loop polls `cv2.waitKey(1)` and honors 'q').\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0440401",
   "metadata": {},
   "source": [
    "## Environment & Dependencies (recommended)\n",
    "\n",
    "This notebook was developed and tested with the following environment (as reported in outputs):\n",
    "- Python 3.10\n",
    "- Ultralytics `8.4.5` (YOLO11n model example)\n",
    "- PyTorch (as reported by `ultralytics` output; e.g. `torch-2.5.1+cu121`)\n",
    "- CUDA-enabled GPU recommended for real-time performance (example used NVIDIA GeForce RTX 3050)\n",
    "\n",
    "Suggested install (example using pip in the conda env):\n",
    "```bash\n",
    "pip install ultralytics==8.4.5 boxmot opencv-python torch torchvision numpy pandas tqdm\n",
    "# plus any project-specific dependencies found in requirements.txt\n",
    "```\n",
    "\n",
    "If you use `conda`, make sure CUDA toolkit and compatible `torch` build are installed for GPU acceleration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0659d4f",
   "metadata": {},
   "source": [
    "## Inputs & Outputs\n",
    "\n",
    "- **Input video file**: set `source` variable to your video path (e.g. `your_traffic_video.mp4`).\n",
    "- **Output video**: `output_counting.mp4` (written using `cv2.VideoWriter`).\n",
    "- **Interactive selectors**: polygon, rectangle, lines, OBB, points ‚Äî use the selectors in `utils.shape_setter` to create ROIs/lines.\n",
    "\n",
    "Notes:\n",
    "- The notebook contains an interactive selection snippet (using OpenCV GUI). If you run this on a remote or headless server, those interactive selectors will not function ‚Äî use a local GUI session.\n",
    "- Ensure the `source` path is readable and `cv2.VideoCapture(source)` works before selecting shapes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef55882f",
   "metadata": {},
   "source": [
    "## Cell-by-cell explanation (map to original cells)\n",
    "\n",
    "I kept the original code cells unchanged. Here is a short description for each block so you can use this notebook as documentation/README:\n",
    "\n",
    "1. `pwd` ‚Äî convenience cell to check the current working directory (original cell preserved).\n",
    "2. `cd ..` ‚Äî example command to change directory (original cell preserved).\n",
    "3. Interactive selector script ‚Äî demonstrates how to open the first frame and run various selectors (PolygonSelector is enabled in the example). Use this to define ROIs, lines, rectangles, OBBs, or single points interactively.\n",
    "4. `### Imprt packages` ‚Äî header cell (markdown) in the original notebook indicating the start of imports.\n",
    "5. Imports and model initialisation ‚Äî imports OpenCV, NumPy, Torch and project modules; initialises `UltralyticsDetector(\"yolo11n.pt\", conf=0.50)`, sets COCO vehicle classes and device.\n",
    "6. A small introspection cell that prints model predictor args (confidence and image size).\n",
    "7. `TorchvisionDetector` snippet ‚Äî shows how to initialize an alternative detector (Faster R-CNN) if desired.\n",
    "8. Ordered counters helper function and counting configs ‚Äî builds `CountVisualizer`, loads counting areas via `CountingConfigLoader`, orders counters for rendering.\n",
    "9. Profilers setup ‚Äî initialises profiling helpers for different pipeline stages (inference, pre/post, tracking, counting, etc.).\n",
    "10. Tracker selection and initialisation ‚Äî example shows `tracking_method = \"bytetrack\"` and creates a `Tracker`.\n",
    "11. Main pipeline runner ‚Äî this is the main loop: opens `FrameGrabber`, initialises `VideoWriter`, iterates over frames, runs `my_model.detect_to_track()`, updates the tracker, counts with configured counters, renders the visualisation using `CountVisualizer`, writes frames to `output_counting.mp4` and shows them in an OpenCV window. The loop is interruptible with the `q` key and properly releases resources in the `finally` block.\n",
    "12. Final small introspection cells that display counts (`g_count`, etc.).\n",
    "\n",
    "If you want me to produce a dedicated `README.md` (Markdown file) extracted from these explanations, tell me and I will generate it as a separate file too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486e3c68",
   "metadata": {},
   "source": [
    "## Important Tips & Troubleshooting\n",
    "\n",
    "- **Interactive GUI:** The selectors and `cv2.imshow` require a desktop session. They will not work on headless servers unless you use a virtual display (e.g., Xvfb on Linux).\n",
    "- **VideoWriter:** The notebook uses `mp4v` fourcc. If output fails to open, confirm codecs are available on your platform. The code asserts `video_writer.isOpened()`.\n",
    "- **Stopping:** Press **`q`** in the OpenCV window to stop early. The `finally` block ensures `frame_grabber.release()`, `video_writer.release()` and `cv2.destroyAllWindows()` are called.\n",
    "- **Paths:** Use absolute paths for `source` to avoid accidental wrong working directory problems.\n",
    "- **GPU:** For real-time performance use CUDA-enabled PyTorch and Ultralytics. Confirm `my_model.predictor.device` shows `cuda`.\n",
    "- **Versioning:** The notebook was run with `Ultralytics 8.4.5`. If you use another version, some APIs or model names might differ.\n",
    "\n",
    "If you encounter errors when running the pipeline, copy the full traceback here and I will help you debug quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494fd1b6",
   "metadata": {},
   "source": [
    "## License & Attribution\n",
    "\n",
    "Keep the notebook's original license headers (if any) and the repository LICENSE file. This documentation is intended to accompany the project and does not change original code authorship.\n",
    "\n",
    "‚Äî End of documentation header ‚Äî original code cells follow unchanged below ‚Äî\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd59a465-da0b-495f-9482-09b8f9819b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb9f29-d997-4129-b5e7-fe3f048200a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05480499-246b-4260-bdff-31af54b81af6",
   "metadata": {},
   "source": [
    "### Use the following script to visulaize and set the geometric shapes (rois polygons, lines,...) you want to set in the con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077755d0-3be9-4338-be11-d3b75db254e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.shape_setter import PointSelector , LineSelector , TwoLineSelector , PolygonSelector , RectangleSelector , OBBSelector\n",
    "import cv2\n",
    "import numpy as np\n",
    "source = r\"you_target_video.mp4\" \n",
    "\n",
    "stride = 1\n",
    "stride_method = \"periodic_stride\"             # \"burst_stride\", \"periodic_stride\", \"random_sampling\"\n",
    "\n",
    "cap = cv2.VideoCapture(source)\n",
    "ok, first_frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "if not ok or first_frame is None:\n",
    "    print(\"Failed to read example frame; please provide a valid path ('kech.mp4' used in example).\")\n",
    "else:\n",
    "    # # line\n",
    "    # line_sel = LineSelector(max_display_size=900, auto_confirm=True, preview_wait_secs=None)\n",
    "    # line = line_sel.select_line(first_frame)\n",
    "    # print(\"Selected line:\", line)\n",
    "\n",
    "    # # two lines\n",
    "    # two_sel = TwoLineSelector(max_display_size=900, auto_confirm=True, preview_wait_secs=None)\n",
    "    # two = two_sel.select_two_lines(first_frame)\n",
    "    # print(\"Selected two lines:\", two)\n",
    "\n",
    "    #polygon\n",
    "    poly_sel = PolygonSelector(max_display_size=900, min_points=4, auto_close_on_click_near_first=True,\n",
    "                               close_pixel_radius=12, preview_wait_secs=None)\n",
    "    poly = poly_sel.select_polygon(first_frame)\n",
    "    print(\"Selected polygon:\", poly)\n",
    "\n",
    "    # # rectangle\n",
    "    # rect_sel = RectangleSelector(max_display_size=900, auto_confirm=True, preview_wait_secs=None)\n",
    "    # rect = rect_sel.select_rectangle(first_frame)\n",
    "    # if rect is None:\n",
    "    #     print(\"Cancelled\")\n",
    "    # else:\n",
    "    #     print(\"Selected rectangle:\", rect)\n",
    "\n",
    "\n",
    "    # # # obb¬£\n",
    "    # obb_selector = OBBSelector()\n",
    "    # obb_points = obb_selector.select_obb(first_frame)\n",
    "    # if obb_points is not None:\n",
    "    #     print(\"Selected OBB points:\", obb_points)\n",
    "    # else:\n",
    "    #     print(\"Selection cancelled\")\n",
    "\n",
    "\n",
    "    # selector = PointSelector()\n",
    "    # pt = selector.select_point(first_frame)\n",
    "    # print(\"Selected point:\", pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00fe414-4522-4cb5-84dd-b0139a4b22ae",
   "metadata": {},
   "source": [
    "### Imprt packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "214d3ddb-65ba-48e3-a7d5-5301a61f2bda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.4.5  Python-3.10.18 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU, 6144MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "Successfully yolo11n.pt model is initialized and warmedup !\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#####################################################################################################################################\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#####################################################################################################################################\n",
    "\n",
    "from framegrabber.frame_grabber import FrameGrabber\n",
    "\n",
    "from detection.ultralytics_detectors import UltralyticsDetector\n",
    "\n",
    "from tracking.track import Tracker\n",
    "\n",
    "from counting.count_config_loader import CountingConfigLoader\n",
    "from counting.count_visualizer import CountVisualizer\n",
    "\n",
    "#####################################################################################################################################\n",
    "\n",
    "# Check the ultralytics repo/website/blogs to see all availaible detectors: just put the name here to use it for vehicle counting\n",
    "my_model = UltralyticsDetector(\"yolo11n.pt\" , conf=0.50)         # rtdetr-l.pt  yolo11n.pt yolo26n.pt yolo11n_finetuned\n",
    "\n",
    "# target classes to be filtered later (ex: vehicles with 4 wheels)\n",
    "coco_vehicles = [1, 2, 3, 5, 7]                              # Bicycle, Car, Motorcycle, Bus and Truck\n",
    "vehicles_4_wheels = [2, 5, 7]                                # Car, Bus and Truck\n",
    "device = my_model.predictor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51fd0da8-ad93-4c38-bbae-fafee711fe64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, [640])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.predictor.args.conf , my_model.predictor.args.imgsz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76f681cc-869b-4baf-b639-442bb3ed32ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In case you want to use torchivision detectors\n",
    "from detection.torchvision_detectors import TorchvisionDetector\n",
    "\n",
    "det = TorchvisionDetector(\n",
    "    \"fasterrcnn_resnet50_fpn_v2\",\n",
    "    conf=0.6,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "# detections = det.detect_to_track(frame.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6230f91d-982b-4d45-a1d7-cea3933eb019",
   "metadata": {},
   "source": [
    "* yolo26n: YOLO26n summary (fused): 122 layers, 2,408,932 parameters, 0 gradients, 5.4 GFLOPs\n",
    "* YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
    "* YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "079f2035-213d-4b17-8c76-d88f71a87eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<counting.count.CountingROIWithIds at 0x1cf3f276d10>,\n",
       " <counting.count.CountingROIWithIds at 0x1cf3f2f5f90>,\n",
       " <counting.count.CountingGlobalAreaWithIds at 0x1cf3f18bac0>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ordered_counters(*counters):\n",
    "    \"\"\"\n",
    "    Accepts:\n",
    "      ([roi1, roi2], global_ctr)\n",
    "      or\n",
    "      ([roi1, roi2, global_ctr],)\n",
    "\n",
    "    Returns:\n",
    "      flat, ordered list of counter objects\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- flatten ----\n",
    "    flat = []\n",
    "    for c in counters:\n",
    "        if isinstance(c, (list, tuple)):\n",
    "            flat.extend(c)\n",
    "        else:\n",
    "            flat.append(c)\n",
    "\n",
    "    # ---- semantic order ----\n",
    "    def key(ctr):\n",
    "        info = ctr.get_area_info()\n",
    "        # ROI / line first, global last\n",
    "        if info.get(\"polygon\") is not None or info.get(\"line\") is not None:\n",
    "            return (0, info.get(\"name\", \"\"))\n",
    "        return (1, info.get(\"name\", \"\"))\n",
    "\n",
    "    return sorted(flat, key=key)\n",
    "\n",
    "\n",
    "# You can set region of interest polygons, lines of counting, paramters of counting, logic of counting and other hyperparameters from the .yaml in the config folder\n",
    "count_vis = CountVisualizer(\n",
    "    show_legend=False,   # hide class legend\n",
    "    show_summary=True    # keep total summary box\n",
    ")\n",
    "\n",
    "# Load counting params as set in the counting yaml config file.\n",
    "counter_load = CountingConfigLoader(default_classes = vehicles_4_wheels)\n",
    "\n",
    "# set counters\n",
    "counters = counter_load.load_counting_areas()\n",
    "\n",
    "# order counters for visualisation\n",
    "counters = ordered_counters(*counters)\n",
    "counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69f54edb-409f-410f-a408-db99b5f50140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiles to diagnonise multiple stages latencies\n",
    "from utils.profilers import Profile\n",
    "\n",
    "device = my_model.predictor.device\n",
    "inf_profile = Profile(device=device)\n",
    "pre_profile = Profile(device=device)\n",
    "post_profile = Profile(device=device)\n",
    "track_profile = Profile()\n",
    "count_profile = Profile()\n",
    "grabber_profile = Profile()\n",
    "plot_profile = Profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c8d86e0-32a9-4469-bc6c-ae5648eefd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Availaible trackers: ocsort, bytetrack, strongsort, deepocsort, hybridsort, boosttrack, botsort\n",
    "tracking_method = \"bytetrack\"\n",
    "tracker = Tracker(tracking_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bd6cfd9-94fd-4809-a3e3-8aec7c7b6647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_grabber index: 0\n",
      "frame_grabber index: 2\n",
      "frame_grabber index: 4\n",
      "frame_grabber index: 6\n",
      "frame_grabber index: 8\n",
      "frame_grabber index: 10\n",
      "frame_grabber index: 12\n",
      "frame_grabber index: 14\n",
      "frame_grabber index: 16\n",
      "frame_grabber index: 18\n",
      "frame_grabber index: 20\n",
      "frame_grabber index: 22\n",
      "frame_grabber index: 24\n",
      "frame_grabber index: 26\n",
      "frame_grabber index: 28\n",
      "frame_grabber index: 30\n",
      "frame_grabber index: 32\n",
      "frame_grabber index: 34\n",
      "frame_grabber index: 36\n",
      "frame_grabber index: 38\n",
      "frame_grabber index: 40\n",
      "frame_grabber index: 42\n",
      "frame_grabber index: 44\n",
      "frame_grabber index: 46\n",
      "frame_grabber index: 48\n",
      "frame_grabber index: 50\n",
      "frame_grabber index: 52\n",
      "frame_grabber index: 54\n",
      "frame_grabber index: 56\n",
      "frame_grabber index: 58\n",
      "frame_grabber index: 60\n",
      "frame_grabber index: 62\n",
      "frame_grabber index: 64\n",
      "frame_grabber index: 66\n",
      "frame_grabber index: 68\n",
      "frame_grabber index: 70\n",
      "frame_grabber index: 72\n",
      "frame_grabber index: 74\n",
      "frame_grabber index: 76\n",
      "frame_grabber index: 78\n",
      "frame_grabber index: 80\n",
      "frame_grabber index: 82\n",
      "frame_grabber index: 84\n",
      "frame_grabber index: 86\n",
      "frame_grabber index: 88\n",
      "frame_grabber index: 90\n",
      "frame_grabber index: 92\n",
      "frame_grabber index: 94\n",
      "frame_grabber index: 96\n",
      "frame_grabber index: 98\n",
      "frame_grabber index: 100\n",
      "frame_grabber index: 102\n",
      "frame_grabber index: 104\n",
      "frame_grabber index: 106\n",
      "frame_grabber index: 108\n",
      "frame_grabber index: 110\n",
      "frame_grabber index: 112\n",
      "frame_grabber index: 114\n",
      "frame_grabber index: 116\n",
      "frame_grabber index: 118\n",
      "frame_grabber index: 120\n",
      "frame_grabber index: 122\n",
      "frame_grabber index: 124\n",
      "frame_grabber index: 126\n",
      "frame_grabber index: 128\n",
      "frame_grabber index: 130\n",
      "frame_grabber index: 132\n",
      "frame_grabber index: 134\n",
      "frame_grabber index: 136\n",
      "frame_grabber index: 138\n",
      "frame_grabber index: 140\n",
      "frame_grabber index: 142\n",
      "frame_grabber index: 144\n",
      "frame_grabber index: 146\n",
      "frame_grabber index: 148\n",
      "frame_grabber index: 150\n",
      "frame_grabber index: 152\n",
      "frame_grabber index: 154\n",
      "frame_grabber index: 156\n",
      "frame_grabber index: 158\n",
      "frame_grabber index: 160\n",
      "frame_grabber index: 162\n",
      "frame_grabber index: 164\n",
      "frame_grabber index: 166\n",
      "frame_grabber index: 168\n",
      "frame_grabber index: 170\n",
      "frame_grabber index: 172\n",
      "frame_grabber index: 174\n",
      "frame_grabber index: 176\n",
      "frame_grabber index: 178\n",
      "frame_grabber index: 180\n",
      "frame_grabber index: 182\n",
      "frame_grabber index: 184\n",
      "frame_grabber index: 186\n",
      "frame_grabber index: 188\n",
      "frame_grabber index: 190\n",
      "frame_grabber index: 192\n",
      "frame_grabber index: 194\n",
      "frame_grabber index: 196\n",
      "frame_grabber index: 198\n",
      "frame_grabber index: 200\n",
      "frame_grabber index: 202\n",
      "frame_grabber index: 204\n",
      "frame_grabber index: 206\n",
      "frame_grabber index: 208\n",
      "frame_grabber index: 210\n",
      "frame_grabber index: 212\n",
      "frame_grabber index: 214\n",
      "frame_grabber index: 216\n",
      "frame_grabber index: 218\n",
      "frame_grabber index: 220\n",
      "frame_grabber index: 222\n",
      "frame_grabber index: 224\n",
      "frame_grabber index: 226\n",
      "frame_grabber index: 228\n",
      "frame_grabber index: 230\n",
      "frame_grabber index: 232\n",
      "frame_grabber index: 234\n",
      "frame_grabber index: 236\n",
      "frame_grabber index: 238\n",
      "frame_grabber index: 240\n",
      "frame_grabber index: 242\n",
      "frame_grabber index: 244\n",
      "frame_grabber index: 246\n",
      "frame_grabber index: 248\n",
      "frame_grabber index: 250\n",
      "frame_grabber index: 252\n",
      "frame_grabber index: 254\n",
      "frame_grabber index: 256\n",
      "frame_grabber index: 258\n",
      "frame_grabber index: 260\n",
      "frame_grabber index: 262\n",
      "frame_grabber index: 264\n",
      "frame_grabber index: 266\n",
      "frame_grabber index: 268\n",
      "frame_grabber index: 270\n",
      "frame_grabber index: 272\n",
      "frame_grabber index: 274\n",
      "frame_grabber index: 276\n",
      "frame_grabber index: 278\n",
      "frame_grabber index: 280\n",
      "frame_grabber index: 282\n",
      "frame_grabber index: 284\n",
      "frame_grabber index: 286\n",
      "frame_grabber index: 288\n",
      "frame_grabber index: 290\n",
      "frame_grabber index: 292\n",
      "frame_grabber index: 294\n",
      "frame_grabber index: 296\n",
      "frame_grabber index: 298\n",
      "frame_grabber index: 300\n",
      "frame_grabber index: 302\n",
      "frame_grabber index: 304\n",
      "frame_grabber index: 306\n",
      "frame_grabber index: 308\n",
      "frame_grabber index: 310\n",
      "frame_grabber index: 312\n",
      "frame_grabber index: 314\n",
      "frame_grabber index: 316\n",
      "frame_grabber index: 318\n",
      "frame_grabber index: 320\n",
      "frame_grabber index: 322\n",
      "frame_grabber index: 324\n",
      "frame_grabber index: 326\n",
      "frame_grabber index: 328\n",
      "frame_grabber index: 330\n",
      "frame_grabber index: 332\n",
      "frame_grabber index: 334\n",
      "frame_grabber index: 336\n",
      "frame_grabber index: 338\n",
      "frame_grabber index: 340\n",
      "frame_grabber index: 342\n",
      "frame_grabber index: 344\n",
      "frame_grabber index: 346\n",
      "frame_grabber index: 348\n",
      "frame_grabber index: 350\n",
      "frame_grabber index: 352\n",
      "frame_grabber index: 354\n",
      "frame_grabber index: 356\n",
      "frame_grabber index: 358\n",
      "frame_grabber index: 360\n",
      "frame_grabber index: 362\n",
      "frame_grabber index: 364\n",
      "frame_grabber index: 366\n",
      "frame_grabber index: 368\n",
      "frame_grabber index: 370\n",
      "frame_grabber index: 372\n",
      "frame_grabber index: 374\n",
      "frame_grabber index: 376\n",
      "frame_grabber index: 378\n",
      "frame_grabber index: 380\n",
      "frame_grabber index: 382\n",
      "frame_grabber index: 384\n",
      "frame_grabber index: 386\n",
      "frame_grabber index: 388\n",
      "frame_grabber index: 390\n",
      "frame_grabber index: 392\n",
      "frame_grabber index: 394\n",
      "frame_grabber index: 396\n",
      "frame_grabber index: 398\n",
      "frame_grabber index: 400\n",
      "frame_grabber index: 402\n",
      "frame_grabber index: 404\n",
      "frame_grabber index: 406\n",
      "frame_grabber index: 408\n",
      "frame_grabber index: 410\n",
      "frame_grabber index: 412\n",
      "frame_grabber index: 414\n",
      "frame_grabber index: 416\n",
      "frame_grabber index: 418\n",
      "frame_grabber index: 420\n",
      "frame_grabber index: 422\n",
      "frame_grabber index: 424\n",
      "frame_grabber index: 426\n",
      "frame_grabber index: 428\n",
      "frame_grabber index: 430\n",
      "frame_grabber index: 432\n",
      "frame_grabber index: 434\n",
      "frame_grabber index: 436\n",
      "frame_grabber index: 438\n",
      "frame_grabber index: 440\n",
      "frame_grabber index: 442\n",
      "frame_grabber index: 444\n",
      "frame_grabber index: 446\n",
      "frame_grabber index: 448\n",
      "frame_grabber index: 450\n",
      "frame_grabber index: 452\n",
      "frame_grabber index: 454\n",
      "frame_grabber index: 456\n",
      "frame_grabber index: 458\n",
      "frame_grabber index: 460\n",
      "frame_grabber index: 462\n",
      "frame_grabber index: 464\n",
      "frame_grabber index: 466\n",
      "frame_grabber index: 468\n",
      "frame_grabber index: 470\n",
      "frame_grabber index: 472\n",
      "frame_grabber index: 474\n",
      "frame_grabber index: 476\n",
      "frame_grabber index: 478\n",
      "frame_grabber index: 480\n",
      "frame_grabber index: 482\n",
      "frame_grabber index: 484\n",
      "frame_grabber index: 486\n",
      "frame_grabber index: 488\n",
      "frame_grabber index: 490\n",
      "frame_grabber index: 492\n",
      "frame_grabber index: 494\n",
      "frame_grabber index: 496\n",
      "frame_grabber index: 498\n",
      "frame_grabber index: 500\n",
      "frame_grabber index: 502\n",
      "frame_grabber index: 504\n",
      "frame_grabber index: 506\n",
      "frame_grabber index: 508\n",
      "frame_grabber index: 510\n",
      "frame_grabber index: 512\n",
      "frame_grabber index: 514\n",
      "frame_grabber index: 516\n",
      "frame_grabber index: 518\n",
      "frame_grabber index: 520\n",
      "frame_grabber index: 522\n",
      "frame_grabber index: 524\n",
      "frame_grabber index: 526\n",
      "frame_grabber index: 528\n",
      "frame_grabber index: 530\n",
      "frame_grabber index: 532\n",
      "frame_grabber index: 534\n",
      "frame_grabber index: 536\n",
      "frame_grabber index: 538\n",
      "frame_grabber index: 540\n",
      "frame_grabber index: 542\n",
      "frame_grabber index: 544\n",
      "frame_grabber index: 546\n",
      "frame_grabber index: 548\n"
     ]
    }
   ],
   "source": [
    "source = r\"your_traffic_video.mp4\"                   # 0 \"kech.mp4\" , \"vid1.mp4\"\n",
    "source = r\"C:\\Users\\hamza\\Datasets\\TrafficDatasets\\IMAROC_2\\kech37.mp4\"\n",
    "# --- Video writer setup ---\n",
    "output_path = \"output_counting.mp4\"\n",
    "\n",
    "fps = 30\n",
    "\n",
    "# Get frame size (wait until first frame if needed)\n",
    "ret, test_cap = cv2.VideoCapture(source).read()\n",
    "h, w = test_cap.shape[:2]\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # widely supported\n",
    "video_writer = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
    "\n",
    "assert video_writer.isOpened(), \"Failed to open VideoWriter\"\n",
    "\n",
    "\n",
    "\n",
    "stride = 2\n",
    "stride_method = \"periodic_stride\"             # \"burst_stride\", \"periodic_stride\", \"random_sampling\"\n",
    "\n",
    "frame_grabber = FrameGrabber(source, stride=stride, stride_method=stride_method)\n",
    "\n",
    "\n",
    "if not frame_grabber.open():\n",
    "    raise RuntimeError(\"Failed to open source\")\n",
    "# ensure window exists (main thread)\n",
    "cv2.namedWindow('BoXMOT + ultralytics', cv2.WINDOW_NORMAL)\n",
    "if frame_grabber._grabber_mode==\"queue\":\n",
    "# start producer\n",
    "    frame_grabber.start()\n",
    "\n",
    "try:\n",
    "    with torch.inference_mode():\n",
    "        while True:\n",
    "            with grabber_profile:\n",
    "                # try to get a frame but don't block forever\n",
    "                frame = frame_grabber.get_frame(timeout=0.1)  # <-- short timeout keeps loop responsive\n",
    "                    \n",
    "            if frame is not None:\n",
    "                print(f\"frame_grabber index: {frame.read_idx}\")\n",
    "\n",
    "                with inf_profile:\n",
    "                    ready_to_track_array = my_model.detect_to_track(frame.data)\n",
    "   \n",
    "                with track_profile:\n",
    "                    res = tracker.update(ready_to_track_array , frame.data)\n",
    "                    # print(f\"tracking array: {res} for counter: {counters[2]}\")\n",
    "\n",
    "                ## Plot detection for ultralytics models\n",
    "                # det_array_plot = my_model.plot()\n",
    "                ## Plot detection for Torchvision models\n",
    "                # det_array_plot = det.plot(frame.data, detections)\n",
    "\n",
    "                # Plot tracks\n",
    "                track_array_plot = tracker.tracker.plot_results(frame.data, show_trajectories=True)\n",
    "\n",
    "                with count_profile:\n",
    "                    g_count = counters[2].count(res)\n",
    "                    roi1_count = counters[0].count(res)\n",
    "                    roi2_count = counters[1].count(res)\n",
    "                    \n",
    "                with plot_profile:\n",
    "                    count_plot = count_vis.render(track_array_plot, *counters)\n",
    "\n",
    "\n",
    "                # --- WRITE FRAME TO VIDEO ---\n",
    "                video_writer.write(count_plot)\n",
    "                    \n",
    "                    \n",
    "                # mark processed & show\n",
    "                frame_grabber.mark_processed(frame)\n",
    "\n",
    "                cv2.imshow('BoXMOT + ultralytics', count_plot)\n",
    "                \n",
    "            else:\n",
    "                # no frame this iteration (timeout), you may choose to display a placeholder\n",
    "                # or simply continue ‚Äî but still poll for key events below\n",
    "                pass\n",
    "\n",
    "            # ALWAYS poll keyboard events so 'q' is detected even when no frame was available\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                # stop producer and break loop\n",
    "                if frame_grabber._grabber_mode==\"queue\":\n",
    "                    frame_grabber.stop(wait=True)\n",
    "                break\n",
    "    \n",
    "                # optional: also break when producer finished (sentinel)\n",
    "                if frame_grabber._grabber_mode==\"queue\":\n",
    "                    if frame is None and frame_grabber._stop_event.is_set():\n",
    "                        break\n",
    "finally:\n",
    "    frame_grabber.release()\n",
    "    video_writer.release()   # <-- ADD THIS\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f551a0-ada1-40b1-b276-f05eefea00b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_count , roi1_count , roi2_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f43721-9021-429f-a20e-b7ad7aa8ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_count.total_count , g_count.counts_by_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
