#!/usr/bin/env python3
"""
sequences_merge.py

Utilities to:
 - create a "big" JSON from per-sequence JSON files + experiments/meta.json
 - update an existing big JSON by adding missing sequence files

Functions:
 - create_big_json_from_sequences(sequences_dir, output_json, meta_path=None, overwrite=False, skip_names=None)
 - update_big_json_with_sequences(sequences_dir, big_json_path, meta_path=None, skip_names=None)

CLI:
    python sequences_merge.py create  --sequences DIR --output OUT.json [--meta META] [--overwrite]
    python sequences_merge.py update  --sequences DIR --big_json EXISTING.json [--meta META]

All path inputs may be strings or pathlib.Path objects.
"""
from __future__ import annotations
import json
import argparse
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Iterable, Tuple, Any, Optional

logger = logging.getLogger("sequences_merge")
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")


# -----------------------
# Helpers
# -----------------------
def load_meta(meta_path: Optional[Path]) -> Dict[str, Any]:
    """Load meta.json if present, otherwise return a fallback meta dict."""
    if meta_path is None:
        return {"timestamp_utc": datetime.utcnow().isoformat() + "Z", "note": "meta.json not provided; autogenerated"}

    meta_path = Path(meta_path)
    if not meta_path.exists():
        logger.warning("meta.json not found: %s — using autogenerated meta", meta_path)
        return {"timestamp_utc": datetime.utcnow().isoformat() + "Z", "note": "meta.json not found; autogenerated"}

    try:
        with meta_path.open("r", encoding="utf-8") as fh:
            return json.load(fh)
    except Exception as e:
        logger.warning("Failed to load meta.json (%s): %s — using fallback meta", meta_path, e)
        return {"timestamp_utc": datetime.utcnow().isoformat() + "Z", "note": "meta.json corrupted; fallback used"}


def collect_sequence_files(sequences_dir: Path, skip_names: Optional[Iterable[str]] = None) -> Iterable[Path]:
    """Return an iterator over sequence JSON files in sequences_dir, skipping names in skip_names."""
    sequences_dir = Path(sequences_dir)
    if not sequences_dir.exists() or not sequences_dir.is_dir():
        raise FileNotFoundError(f"Sequences directory not found or not a directory: {sequences_dir}")

    skip_names = {s.lower() for s in (skip_names or ("meta.json", "meta.tmp.json"))}
    for p in sorted(sequences_dir.glob("*.json")):
        if p.name.lower() in skip_names:
            logger.debug("Skipping meta/temp-like file: %s", p)
            continue
        yield p


def read_json_file(path: Path) -> Any:
    """Read and parse a JSON file; raise on failure."""
    with path.open("r", encoding="utf-8") as fh:
        return json.load(fh)


def atomic_write_json(target_path: Path, data: Any) -> None:
    """
    Atomically write JSON to target_path by writing to a temp file in the same directory
    and replacing the target file.
    """
    target_path = Path(target_path)
    tmp_path = target_path.parent / (target_path.name + ".tmp")
    try:
        with tmp_path.open("w", encoding="utf-8") as fh:
            json.dump(data, fh, ensure_ascii=False, indent=2)
            fh.flush()
        tmp_path.replace(target_path)
    finally:
        if tmp_path.exists():
            try:
                tmp_path.unlink()
            except Exception:
                # best-effort cleanup; don't raise
                pass


# -----------------------
# High-level operations
# -----------------------
def create_big_json_from_sequences(
    sequences_dir: Path,
    output_json: Path,
    meta_path: Optional[Path] = None,
    overwrite: bool = False,
    skip_names: Optional[Iterable[str]] = None,
) -> Dict[str, int]:
    """
    Create a new big JSON file by merging all sequence JSON files under sequences_dir
    and including `meta.json` (if provided).

    Returns a summary dict: {"added": int, "skipped": int, "failed": int}
    """
    sequences_dir = Path(sequences_dir)
    output_json = Path(output_json)

    if output_json.exists() and not overwrite:
        raise FileExistsError(f"Output JSON already exists: {output_json} (use overwrite=True to replace)")

    meta = load_meta(Path(meta_path) if meta_path else (sequences_dir.parent / "meta.json"))
    big_data = {"meta": meta, "sequences": {}}

    added = skipped = failed = 0
    for seq_file in collect_sequence_files(sequences_dir, skip_names=skip_names):
        seq_key = seq_file.stem
        try:
            seq_obj = read_json_file(seq_file)
        except Exception as e:
            logger.warning("Failed to read/parse %s: %s", seq_file, e)
            failed += 1
            continue

        if seq_key in big_data["sequences"]:
            logger.debug("Duplicate sequence key encountered, skipping: %s", seq_key)
            skipped += 1
            continue

        big_data["sequences"][seq_key] = seq_obj
        added += 1

    atomic_write_json(output_json, big_data)
    logger.info("[DONE] Added %d sequences, skipped %d, failed %d.", added, skipped, failed)
    logger.info("[OK] Big JSON created: %s", output_json)
    return {"added": added, "skipped": skipped, "failed": failed}


def update_big_json_with_sequences(
    sequences_dir: Path,
    big_json_path: Path,
    meta_path: Optional[Path] = None,
    skip_names: Optional[Iterable[str]] = None,
) -> Dict[str, int]:
    """
    Update an existing big JSON by adding sequences found under sequences_dir that are not already present.

    Returns a summary dict: {"added": int, "skipped": int, "failed": int}
    """
    sequences_dir = Path(sequences_dir)
    big_json_path = Path(big_json_path)

    if not big_json_path.exists():
        raise FileNotFoundError(f"Big JSON not found: {big_json_path}")

    with big_json_path.open("r", encoding="utf-8") as fh:
        big_data = json.load(fh)

    big_data.setdefault("sequences", {})
    # Optionally update meta if a meta_path is provided
    if meta_path:
        big_data["meta"] = load_meta(Path(meta_path))

    added = skipped = failed = 0
    for seq_file in collect_sequence_files(sequences_dir, skip_names=skip_names):
        seq_key = seq_file.stem
        if seq_key in big_data["sequences"]:
            skipped += 1
            continue

        try:
            seq_obj = read_json_file(seq_file)
        except Exception as e:
            logger.warning("Failed to read/parse %s: %s", seq_file, e)
            failed += 1
            continue

        big_data["sequences"][seq_key] = seq_obj
        added += 1

    atomic_write_json(big_json_path, big_data)
    logger.info("[DONE] Added %d sequences, skipped %d, failed %d.", added, skipped, failed)
    logger.info("[OK] Big JSON updated: %s", big_json_path)
    return {"added": added, "skipped": skipped, "failed": failed}


# -----------------------
# CLI
# -----------------------
def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Create or update a big JSON from per-sequence JSON files.")
    sub = p.add_subparsers(dest="cmd", required=True)

    create = sub.add_parser("create", help="Create a new big JSON from sequences")
    create.add_argument("--sequences", "-s", required=True, help="Directory containing per-sequence JSON files")
    create.add_argument("--output", "-o", required=True, help="Path for the output big JSON")
    create.add_argument("--meta", "-m", default=None, help="Path to meta.json (optional)")
    create.add_argument("--overwrite", action="store_true", help="Overwrite output if it exists")

    update = sub.add_parser("update", help="Update an existing big JSON with sequences")
    update.add_argument("--sequences", "-s", required=True, help="Directory containing per-sequence JSON files")
    update.add_argument("--big_json", "-b", required=True, help="Path to existing big JSON to update")
    update.add_argument("--meta", "-m", default=None, help="Path to meta.json to replace/update meta in big JSON (optional)")

    p.add_argument("--debug", action="store_true", help="Enable debug logging")
    return p.parse_args()


def main() -> None:
    args = _parse_args()
    if args.debug:
        logger.setLevel(logging.DEBUG)

    if args.cmd == "create":
        sequences = Path(args.sequences)
        output = Path(args.output)
        meta = Path(args.meta) if args.meta else None
        try:
            res = create_big_json_from_sequences(sequences, output, meta_path=meta, overwrite=args.overwrite)
        except Exception as e:
            logger.error("Failed to create big JSON: %s", e)
            raise
        else:
            logger.debug("create result: %s", res)

    elif args.cmd == "update":
        sequences = Path(args.sequences)
        big_json = Path(args.big_json)
        meta = Path(args.meta) if args.meta else None
        try:
            res = update_big_json_with_sequences(sequences, big_json, meta_path=meta)
        except Exception as e:
            logger.error("Failed to update big JSON: %s", e)
            raise
        else:
            logger.debug("update result: %s", res)


if __name__ == "__main__":
    main()
